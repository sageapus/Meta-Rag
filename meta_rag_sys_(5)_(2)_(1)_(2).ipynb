{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sageapus/Meta-Rag/blob/main/meta_rag_sys_(5)_(2)_(1)_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMQlEuNIwxM"
      },
      "source": [
        "# **Welcome to Your First META-Powered Document Assistant**\n",
        "\n",
        "Today, we‚Äôre going to build a simple but powerful AI tool:\n",
        "a PDF Question-and-Answer system powered by Meta‚Äôs Llama AI models.\n",
        "\n",
        "### **This tool will allow you to:**\n",
        "\n",
        "Upload any PDF\n",
        "\n",
        "For example:\n",
        "\n",
        "*   A company report\n",
        "*   A curriculum\n",
        "*   A policy document\n",
        "*   A research paper\n",
        "\n",
        "\n",
        "**Ask natural questions**\n",
        "\n",
        "Like:\n",
        "\n",
        "\n",
        "*   ‚ÄúWhat are the main goals in this document?‚Äù\n",
        "*   ‚ÄúWhat does this policy say about attendance?‚Äù\n",
        "*   ‚ÄúSummarise the key points in chapter one.‚Äù\n",
        "\n",
        "\n",
        "ü§ñ And get **accurate** answers‚Äîbased only on the **PDF** itself\n",
        "\n",
        "The AI does not guess or make things up.\n",
        "It searches the document, finds the right information, and gives you a clean answer.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU-r9ThxURTv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w9qOFy1SM_M"
      },
      "source": [
        "# **Before You Begin**\n",
        "\n",
        "Please ensure you have the following:\n",
        "\n",
        "1. Access to Google Colab\n",
        "\n",
        "2. OpenRouter API Key - https://www.youtube.com/watch?v=-X9DVzzxpAA\n",
        "\n",
        "3. Google Drive link to the PDF\n",
        "\n",
        "ü§ñ - \"May the code be with you\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HYsx_yXvmW"
      },
      "source": [
        "# Step 1 ‚Äî Installing the Tools We Need\n",
        "\n",
        "Before we can start building our AI system, we need to install a few essential tools. Think of this like installing apps on your phone before you can use them.\n",
        "\n",
        "**What you'll need:**\n",
        "\n",
        "1.   **LlamaIndex** ‚Äî helps the AI read and understand documents\n",
        "2.   **OpenRouter connector** ‚Äî allows us to use Meta‚Äôs Llama AI model\n",
        "3.   **HuggingFace Embeddings** ‚Äî helps the AI understand text by turning words into numbers\n",
        "4.  **Document readers** ‚Äî so the notebook can open and read PDF files\n",
        "5.  **Fusion retriever tools** ‚Äî advanced search tools that improve answer accuracy\n",
        "\n",
        "This cell does not build the system yet ‚Äî it only prepares the environment.\n",
        "Once everything is installed, the rest of the notebook will work smoothly.\n",
        "\n",
        "After you run this cell once, you don‚Äôt need to install again unless the runtime resets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX2Ih-3_JOmd"
      },
      "outputs": [],
      "source": [
        "# Step 1 ‚Äì Install all the tools our AI document assistant needs\n",
        "#\n",
        "# In this cell, we install the main libraries that make our RAG (Retrieval-Augmented Generation)\n",
        "# system work. Think of this as installing the ‚Äúapps‚Äù our notebook will use.\n",
        "#\n",
        "# Here is what each tool does:\n",
        "#\n",
        "# llama-index\n",
        "#   - The main framework we use to read documents, break them into pieces, index them,\n",
        "#     and search through them. It handles most of the heavy lifting for our RAG system.\n",
        "#\n",
        "# llama-index-llms-openrouter\n",
        "#   - Allows us to connect to Meta‚Äôs Llama models through OpenRouter, so our AI can\n",
        "#     understand questions and generate answers.\n",
        "#\n",
        "# llama-index-embeddings-huggingface\n",
        "#   - Helps convert text into embeddings (numbers that the AI can understand).\n",
        "#     This is what allows the system to find the right parts of the PDF when you ask a question.\n",
        "#\n",
        "# llama-index-readers-file\n",
        "#   - Gives the notebook the ability to read PDF files and other documents from your system.\n",
        "#\n",
        "# llama-index-packs-fusion-retriever\n",
        "#   - Provides the ‚Äúquery fusion‚Äù tool we use to improve answer accuracy. It takes your question,\n",
        "#     rewrites it in different ways, searches the document multiple times, and combines the results.\n",
        "#\n",
        "# sentence-transformers\n",
        "#   - A library that helps with semantic understanding. It is used for intelligent document chunking.\n",
        "#\n",
        "# nest-asyncio\n",
        "#   - Allows certain parts of the system to run smoothly inside Google Colab by letting\n",
        "#     asynchronous code work without errors.\n",
        "#\n",
        "# requests\n",
        "#   - Helps us download the PDF directly from a Google Drive link.\n",
        "#\n",
        "# After running this cell, all the required tools will be installed and ready.\n",
        "# You only need to run this once per session.\n",
        "#\n",
        "\n",
        "%pip install -q \\\n",
        "  llama-index \\\n",
        "  llama-index-llms-openrouter \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-packs-fusion-retriever \\\n",
        "  sentence-transformers \\\n",
        "  nest-asyncio \\\n",
        "  requests\n",
        "\n",
        "print(\"‚úÖ Installation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQP37F-LZhu_"
      },
      "source": [
        "# Step 2 ‚Äî Setting Up the AI Model\n",
        "\n",
        "Now that all our tools have been installed, this next cell is where we connect our notebook to the actual AI model that will answer our questions.\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "‚úî 1. Enter our OpenRouter API key\n",
        "\n",
        "‚úî 2. Choose which Llama model we want to use\n",
        "\n",
        "‚úî 3. Set rules for how the AI should behave\n",
        "\n",
        "‚úî 4. Set up the embedding model\n",
        "\n",
        "This is a tool that helps the AI ‚Äúunderstand‚Äù text by turning it into numerical patterns.\n",
        "It improves how well the system can find information inside the PDF.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Make sure you import all the required tools and libraries in the cell below‚Äîthese are essential for building your RAG system.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Dc1zsHJOqW"
      },
      "outputs": [],
      "source": [
        " # Step 2 ‚Äì Connect to the AI model and set up how our system will think\n",
        "#\n",
        "# In this cell, we prepare the AI model that will answer our questions.\n",
        "# Everything here is about giving the AI access, choosing the model we want,\n",
        "# and making sure it behaves correctly.\n",
        "\n",
        "# ENTER CODE HERE (IMPORTS)\n",
        "import os\n",
        "from getpass import getpass\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "# Ask the user to enter their OpenRouter API key.\n",
        "# This works like a password that allows us to use the Llama AI model.\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter API key: \")\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Here we choose which Llama model we want to use.\n",
        "# - This model understands the questions we ask\n",
        "# - It helps generate clear, readable answers\n",
        "llm = OpenRouter(\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],  # Connect using our key\n",
        "    model=\"meta-llama/llama-3.3-70b-instruct:free\",  # Choose the AI model\n",
        "    max_tokens=512,  # Maximum size of the answer\n",
        "    temperature=0.1, # Lower = more accurate and less creative\n",
        "    timeout=60,      # Give the model time to respond\n",
        "    system_prompt=(\n",
        "        # These are the rules we give the AI.\n",
        "        # They keep the answers short, accurate, and based ONLY on the PDF.\n",
        "\n",
        "        \"You are an expert RAG system that answers ONLY using the provided context. \"\n",
        "        \"Never hallucinate. Never guess. If the answer is not in the context, say so. \"\n",
        "        \"Provide short, clear, factual responses with 2‚Äì4 evidence bullets.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# This model helps the AI understand text in the PDF by turning words into numbers.\n",
        "# This step helps the system search the document more effectively.\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# Finally, we tell the system to use the AI model and text-understanding model\n",
        "# we just set up. This makes everything ready for the next steps.\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print(\"‚úÖ AI model and settings are ready to use\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "busFqjtTg50p"
      },
      "source": [
        "# Step 3 ‚Äî Loading the PDF From Google Drive\n",
        "\n",
        "In this step, we give our AI system the actual PDF document it needs to learn from.\n",
        "Instead of uploading a file manually, we simply paste a Google Drive link, and the notebook automatically downloads the PDF for us.\n",
        "\n",
        "**This cell:**\n",
        "\n",
        "‚úî 1. Asks you to paste the Google Drive link\n",
        "\n",
        "This is where your PDF is stored.\n",
        "\n",
        "‚úî 2. Extracts the file ID from the link\n",
        "\n",
        "The notebook figures out which file to download.\n",
        "\n",
        "‚úî 3. Downloads the PDF into the notebook\n",
        "\n",
        "Now the AI can read it.\n",
        "\n",
        "‚úî 4. Saves it in a folder so we can use it later\n",
        "\n",
        "This is the document the entire system will work with.\n",
        "\n",
        "**In simple terms:**\n",
        "\n",
        "**Step 3** brings the **PDF** into our **AI workspace** so the system can read it, understand it, and answer questions about it.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "We need to download your PDF document, so please include a function that handles this for you.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "\n",
        "```\n",
        "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
        "    # Try to extract the file ID from the Google Drive link.\n",
        "    # The file ID is the unique part of the link that tells Google which file to download.\n",
        "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "    else:\n",
        "        # If the link is in a different format (?id=...), extract the ID from there instead.\n",
        "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "        else:\n",
        "            # If no file ID is found, let the user know the link is invalid.\n",
        "            raise ValueError(\"‚ùå Could not extract file ID from the link.\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY04rEPsJOwm"
      },
      "outputs": [],
      "source": [
        "# Step 3 ‚Äì Download the PDF from a Google Drive link\n",
        "#\n",
        "# This cell takes the Google Drive link you paste in, downloads the PDF,\n",
        "# and saves it so our AI system can read it later.\n",
        "# You don‚Äôt need to upload anything manually ‚Äî the notebook does it for you.\n",
        "\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# ENTER CODE HERE (Download PDF Function)\n",
        "def download_pdf_from_drive(drive_url: str, save_path: str):\n",
        "    # Try to extract the file ID from the Google Drive link.\n",
        "    # The file ID is the unique part of the link that tells Google which file to download.\n",
        "    match = re.search(r\"/d/([A-Za-z0-9_-]+)\", drive_url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "    else:\n",
        "        # If the link is in a different format (?id=...), extract the ID from there instead.\n",
        "        match = re.search(r\"id=([A-Za-z0-9_-]+)\", drive_url)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "        else:\n",
        "            # If no file ID is found, let the user know the link is invalid.\n",
        "            raise ValueError(\"‚ùå Could not extract file ID from the link.\")\n",
        "\n",
        "    # Build the direct download link for Google Drive using the file ID.\n",
        "    download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "    print(f\"üì• Downloading PDF (file ID {file_id})...\")\n",
        "\n",
        "    # Download the actual PDF file from Google Drive.\n",
        "    resp = requests.get(download_url)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    # Save the downloaded PDF to the folder we created.\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        f.write(resp.content)\n",
        "\n",
        "    print(f\"‚úÖ PDF downloaded ‚Üí {save_path}\")\n",
        "\n",
        "\n",
        "# Ask the user to paste the Google Drive link of the PDF they want to use.\n",
        "drive_link = input(\"üìå Paste your Google Drive PDF link here: \").strip()\n",
        "\n",
        "# Create a folder called 'data' if it does not exist already.\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Save the downloaded PDF as 'source.pdf' inside the data folder.\n",
        "pdf_path = os.path.join(DATA_DIR, \"source.pdf\")\n",
        "\n",
        "# Download the PDF using the link provided.\n",
        "download_pdf_from_drive(drive_link, pdf_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVgSNOG-i0NR"
      },
      "source": [
        "# Step 4 ‚Äî Preparing the PDF for the AI to Understand\n",
        "\n",
        "Now that we‚Äôve downloaded the PDF, the next step is to help the AI make sense of it.\n",
        "\n",
        "AI models cannot read a full PDF the way humans do, so we need to break the document into small, meaningful sections. This step is extremely important because it determines how accurately the AI will find answers later.\n",
        "\n",
        "**In this cell, we:**\n",
        "\n",
        "‚úî 1. Load the PDF into the notebook\n",
        "\n",
        "The system opens the file so it can start reading the content.\n",
        "\n",
        "‚úî 2. Break the PDF into ‚Äúsemantic chunks‚Äù\n",
        "\n",
        "Instead of cutting the document randomly, we break it into smart, meaningful pieces ‚Äî almost like dividing a book into paragraphs and sections.\n",
        "This helps the AI understand the document in a more natural way.\n",
        "\n",
        "‚úî 3. Add simple labels to each chunk\n",
        "\n",
        "This makes it easier for the AI to keep track of where each piece came from.\n",
        "\n",
        "‚úî 4. Prepare these pieces for searching\n",
        "\n",
        "These chunks will later be used by the AI to look up the right answer when you ask a question.\n",
        "\n",
        "**In simple terms:**\n",
        "\n",
        "**Step 4** teaches the AI how to read the PDF in a way it can **understand** and **search** through **accurately**.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "Insert the code that will allow you to create a semantic chunker.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# We use an embedding model to help the AI understand the meaning of the text.\n",
        "# This model helps the system decide where natural breaks (sections) should be.\n",
        "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Create the tool that will split the PDF into smart, meaningful pieces.\n",
        "# Instead of cutting randomly, it looks at the meaning of each sentence\n",
        "# so the chunks feel more natural (like real sections).\n",
        "parser = SemanticSplitterNodeParser(\n",
        "    buffer_size=3,                     # Helps keep small related groups together\n",
        "    breakpoint_percentile_threshold=95, # Controls how sensitive the splitting is\n",
        "    embed_model=semantic_embed          # Uses the meaning of the text for splitting\n",
        ")\n",
        "\n",
        "# Use the parser to turn the document into these small, meaningful chunks.\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPGwzpiCJO3Y"
      },
      "outputs": [],
      "source": [
        "# Cell 4 ‚Äì Break the PDF into meaningful pieces the AI can understand\n",
        "#\n",
        "# In this step, we take the PDF we downloaded and prepare it so the AI can\n",
        "# read it properly. The AI cannot understand one big block of text, so we\n",
        "# break the document into smaller, meaningful sections (called ‚Äúchunks‚Äù).\n",
        "#\n",
        "# These chunks act like paragraphs or mini-sections that the AI can search\n",
        "# through when answering your questions.\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Load the PDF from the folder where we saved it.\n",
        "# This reads the entire document into the system.\n",
        "documents = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\n",
        "print(f\"üìÑ Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# ENTER CODE HERE (SEMANTIC CHUNKER)\n",
        "\n",
        "# We use an embedding model to help the AI understand the meaning of the text.\n",
        "# This model helps the system decide where natural breaks (sections) should be.\n",
        "semantic_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Create the tool that will split the PDF into smart, meaningful pieces.\n",
        "# Instead of cutting randomly, it looks at the meaning of each sentence\n",
        "# so the chunks feel more natural (like real sections).\n",
        "parser = SemanticSplitterNodeParser(\n",
        "    buffer_size=3,                     # Helps keep small related groups together\n",
        "    breakpoint_percentile_threshold=95, # Controls how sensitive the splitting is\n",
        "    embed_model=semantic_embed          # Uses the meaning of the text for splitting\n",
        ")\n",
        "\n",
        "# Use the parser to turn the document into these small, meaningful chunks.\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add simple labels to each chunk so we know where it came from.\n",
        "# This is helpful when the AI searches for answers.\n",
        "for n in nodes:\n",
        "    n.metadata[\"source\"]     = pdf_path\n",
        "    n.metadata[\"chunk_type\"] = \"semantic\"\n",
        "\n",
        "print(f\"üîç Created {len(nodes)} high-quality semantic nodes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUX5lJUkWKo"
      },
      "source": [
        "# Step 5 ‚Äî Building the AI Search Engine\n",
        "\n",
        "Now that our PDF has been broken into small, meaningful pieces, we are ready to build the ‚Äúsearch engine‚Äù part of our AI system. This is one of the most important steps.\n",
        "\n",
        "**In this cell, we put together the tool that helps the AI:**\n",
        "\n",
        "‚úî Understand your question\n",
        "\n",
        "‚úî Rewrite your question in different ways\n",
        "\n",
        "‚úî Search through the PDF multiple times\n",
        "\n",
        "‚úî Combine the best results\n",
        "\n",
        "‚úî Give you the most accurate answer possible\n",
        "\n",
        "This technique is called **Query Fusion**, and it comes from **Meta‚Äôs Llama Cookbook**.\n",
        "It makes the AI much smarter and more reliable‚Äîespecially when questions are complicated or phrased in different ways.\n",
        "\n",
        "Think of it like having several researchers all look for the answer separately, then compare findings and give you the clearest final response.\n",
        "\n",
        "**In simple terms:**\n",
        "\n",
        "**Step 5** builds the brain of our search system ‚Äî the part that helps the AI find the right information in the PDF, even if the question is difficult or worded differently.\n",
        "\n",
        "**Instrutions:**\n",
        "\n",
        "Set up the advanced retriever that will improve your RAG system‚Äôs accuracy by rewriting queries, performing multiple searches, and combining the results.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Build the advanced search system using our document chunks.\n",
        "# Here we set how many searches it should perform and how much information to gather.\n",
        "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
        "    nodes,                        # The chunks created in Step 4\n",
        "    chunk_size=256,               # The size of each text chunk to consider\n",
        "    vector_similarity_top_k=8,    # How many top matches to look at in the first layer of search\n",
        "    fusion_similarity_top_k=8,    # How many matches to merge for the final answer\n",
        "    num_queries=6,                # Rewrites your question 6 different ways for a stronger search\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIChOblhJPKp"
      },
      "outputs": [],
      "source": [
        "# Cell 5 ‚Äì Build the smart search system (Query Fusion)\n",
        "#\n",
        "# Now that we have broken the PDF into small, meaningful pieces (chunks),\n",
        "# this cell builds the ‚Äúsearch engine‚Äù that will help the AI find the\n",
        "# right information when you ask a question.\n",
        "#\n",
        "# This uses a method called **Query Fusion**, which comes from Meta‚Äôs Llama Cookbook.\n",
        "# Query Fusion improves accuracy by:\n",
        "#   - Rewriting your question in different ways\n",
        "#   - Searching the document several times\n",
        "#   - Combining the best results into one strong answer\n",
        "#\n",
        "# Think of it like asking several people the same question and then blending\n",
        "# their answers to get the most reliable one.\n",
        "\n",
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "# Download the special Query Fusion tool.\n",
        "# This only downloads the first time ‚Äî after that it loads from your system.\n",
        "QueryRewritingRetrieverPack = download_llama_pack(\n",
        "    \"QueryRewritingRetrieverPack\",\n",
        "    \"./query_rewriting_pack\",\n",
        ")\n",
        "\n",
        "# ENTER CODE HERE (ADVANCED RETRIEVER CREATION)\n",
        "\n",
        "# Build the advanced search system using our document chunks.\n",
        "# Here we set how many searches it should perform and how much information to gather.\n",
        "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
        "    nodes,                        # The chunks created in Step 4\n",
        "    chunk_size=256,               # The size of each text chunk to consider\n",
        "    vector_similarity_top_k=8,    # How many top matches to look at in the first layer of search\n",
        "    fusion_similarity_top_k=8,    # How many matches to merge for the final answer\n",
        "    num_queries=6,                # Rewrites your question 6 different ways for a stronger search\n",
        ")\n",
        "\n",
        "\n",
        "print(\"üöÄ Advanced Query Fusion RAG Engine Ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aExEZBNmet3"
      },
      "source": [
        "# Step 6 ‚Äî Asking Questions and Getting Answers\n",
        "\n",
        "This is the **final** step ‚Äî and the part you‚Äôll interact with the most.\n",
        "\n",
        "Now that our AI system is fully set up, this cell creates a simple chat-like loop where you can:\n",
        "\n",
        "‚úî Type any question about your PDF\n",
        "\n",
        "‚úî Watch the AI search the document\n",
        "\n",
        "‚úî Receive a clear, accurate answer based only on the PDF\n",
        "\n",
        "‚úî Keep asking as many questions as you want\n",
        "\n",
        "‚úî Type ‚Äúend‚Äù when you‚Äôre finished\n",
        "\n",
        "**Behind the scenes, the AI is:**\n",
        "\n",
        "1.  Reading your question\n",
        "\n",
        "2.  Rewriting it in different ways\n",
        "\n",
        "3.  Searching the PDF for relevant pieces\n",
        "\n",
        "4.  Comparing the results\n",
        "\n",
        "5.  Giving you the best possible answer\n",
        "\n",
        "This turns your PDF into a smart assistant that can explain, summarise, or find information instantly.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "**Step 6** is where you finally get to talk to the AI. You ask questions, it gives answers, and the **conversation** continues until you decide to stop.\n",
        "\n",
        "**Instruction:**\n",
        "\n",
        "Create the interactive loop that allows users to ask questions and receive answers from your RAG system in real time.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This loop keeps running until the user types \"end\".\n",
        "while True:\n",
        "    # Ask the user to type a question.\n",
        "    user_question = input(\"üü¶ Enter your question: \").strip()\n",
        "\n",
        "    # If the user types \"end\", stop the loop and exit.\n",
        "    if user_question.lower() == \"end\":\n",
        "        print(\"\\nüëã Session ended.\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nüîç Retrieving answer...\\n\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IP8xjLhW_jG"
      },
      "outputs": [],
      "source": [
        "# Cell 6 ‚Äì Ask questions and get answers from your PDF\n",
        "#\n",
        "# This final cell creates a simple question-and-answer loop.\n",
        "# You can type any question about your PDF, and the AI will search the document\n",
        "# and give you a clear, accurate answer based only on what is written in the PDF.\n",
        "#\n",
        "# The loop continues until you type \"end\", which stops the system.\n",
        "\n",
        "def safe_rag_run(question, retries=3):\n",
        "    # This function safely runs the AI search.\n",
        "    # If something goes wrong (like a slow internet response),\n",
        "    # it will try again up to 3 times before giving up.\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Ask the AI to answer the question based on the PDF content.\n",
        "            resp = query_rewriting_pack.run(question)\n",
        "\n",
        "            # If the AI returns nothing, treat it as an error.\n",
        "            if resp is None or str(resp).strip() == \"\":\n",
        "                raise ValueError(\"Empty LLM response.\")\n",
        "\n",
        "            return resp\n",
        "\n",
        "        except Exception as e:\n",
        "            # If something goes wrong, show the error and retry.\n",
        "            print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "            print(f\"üîÅ Retrying ({attempt+1}/{retries})...\")\n",
        "\n",
        "    # If all retries fail, return a message saying we couldn‚Äôt get an answer.\n",
        "    return \"‚ùå Could not generate a valid answer after retries.\"\n",
        "\n",
        "print(\"\\nRAG Interactive Mode\")\n",
        "print(\"Ask any question about your PDF.\")\n",
        "print(\"Type 'end' to exit.\\n\")\n",
        "\n",
        "# ENTER CODE HERE (INTERACTIVE LOOP)\n",
        "\n",
        "\n",
        "\n",
        "# This loop keeps running until the user types \"end\".\n",
        "while True:\n",
        "    # Ask the user to type a question.\n",
        "    user_question = input(\"üü¶ Enter your question: \").strip()\n",
        "\n",
        "    # If the user types \"end\", stop the loop and exit.\n",
        "    if user_question.lower() == \"end\":\n",
        "        print(\"\\nüëã Session ended.\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nüîç Retrieving answer...\\n\")\n",
        "\n",
        "\n",
        "    # Run the question through our safe search function.\n",
        "    response = safe_rag_run(user_question)\n",
        "\n",
        "    # Display the question and the AI‚Äôs answer.\n",
        "    print(\"\\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "    print(\"‚ùì QUESTION:\")\n",
        "    print(user_question)\n",
        "    print(\"\\nüß† ANSWER:\")\n",
        "    print(response)\n",
        "    print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}